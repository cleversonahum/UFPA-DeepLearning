{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.17.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN class for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize, nhiddenLayers, hiddenNeurons):\n",
    "        super(linearRegression, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        for i in range(0,nhiddenLayers): # adding hidden layers\n",
    "            if(i==0): #Input layer\n",
    "                layers.append(torch.nn.Linear(inputSize, hiddenNeurons))\n",
    "                layers.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                layers.append(torch.nn.BatchNorm1d(hiddenNeurons))\n",
    "                layers.append(torch.nn.Linear(hiddenNeurons, hiddenNeurons))\n",
    "                layers.append(torch.nn.ReLU())\n",
    "\n",
    "        layers.append(torch.nn.Linear(hiddenNeurons, outputSize)) # output layer\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "        print(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.001\n",
    "momentum = 0.9 \n",
    "epochs = 2000\n",
    "batch_size = 3000\n",
    "train_set = 60000\n",
    "test_set = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = lambda x: 2 * (x ** 2) - 1\n",
    "func = lambda x: f1(f1(np.cos(x)))\n",
    "x = torch.unsqueeze(torch.FloatTensor(train_set+test_set).uniform_(-2*pi, 2*pi), dim=1)\n",
    "y = func(x)\n",
    "\n",
    "# Train Dataset and making generator for mini-batches\n",
    "train_x = x[0:train_set]\n",
    "train_y = y[0:train_set]\n",
    "train_dataset = Data.TensorDataset(train_x, train_y)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test Dataset\n",
    "test_x = x[train_set:(train_set+test_set)]\n",
    "test_y = y[train_set:(train_set+test_set)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Net, Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=24, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = linearRegression(1,1,3, 24)\n",
    "criterion = torch.nn.MSELoss() # Define criterion to evaluate the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learningRate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0   Step:  19   Loss:  tensor(0.4982, grad_fn=<MseLossBackward>)\n",
      "Epoch:  10   Step:  19   Loss:  tensor(0.4463, grad_fn=<MseLossBackward>)\n",
      "Epoch:  20   Step:  19   Loss:  tensor(0.4302, grad_fn=<MseLossBackward>)\n",
      "Epoch:  30   Step:  19   Loss:  tensor(0.3965, grad_fn=<MseLossBackward>)\n",
      "Epoch:  40   Step:  19   Loss:  tensor(0.3700, grad_fn=<MseLossBackward>)\n",
      "Epoch:  50   Step:  19   Loss:  tensor(0.3204, grad_fn=<MseLossBackward>)\n",
      "Epoch:  60   Step:  19   Loss:  tensor(0.2726, grad_fn=<MseLossBackward>)\n",
      "Epoch:  70   Step:  19   Loss:  tensor(0.2288, grad_fn=<MseLossBackward>)\n",
      "Epoch:  80   Step:  19   Loss:  tensor(0.2138, grad_fn=<MseLossBackward>)\n",
      "Epoch:  90   Step:  19   Loss:  tensor(0.1772, grad_fn=<MseLossBackward>)\n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.1670, grad_fn=<MseLossBackward>)\n",
      "Epoch:  110   Step:  19   Loss:  tensor(0.0401, grad_fn=<MseLossBackward>)\n",
      "Epoch:  120   Step:  19   Loss:  tensor(0.0585, grad_fn=<MseLossBackward>)\n",
      "Epoch:  130   Step:  19   Loss:  tensor(0.0564, grad_fn=<MseLossBackward>)\n",
      "Epoch:  140   Step:  19   Loss:  tensor(0.0166, grad_fn=<MseLossBackward>)\n",
      "Epoch:  150   Step:  19   Loss:  tensor(0.0346, grad_fn=<MseLossBackward>)\n",
      "Epoch:  160   Step:  19   Loss:  tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "Epoch:  170   Step:  19   Loss:  tensor(0.0111, grad_fn=<MseLossBackward>)\n",
      "Epoch:  180   Step:  19   Loss:  tensor(0.0135, grad_fn=<MseLossBackward>)\n",
      "Epoch:  190   Step:  19   Loss:  tensor(0.0148, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.0178, grad_fn=<MseLossBackward>)\n",
      "Epoch:  210   Step:  19   Loss:  tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "Epoch:  220   Step:  19   Loss:  tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "Epoch:  230   Step:  19   Loss:  tensor(0.0097, grad_fn=<MseLossBackward>)\n",
      "Epoch:  240   Step:  19   Loss:  tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Epoch:  250   Step:  19   Loss:  tensor(0.0143, grad_fn=<MseLossBackward>)\n",
      "Epoch:  260   Step:  19   Loss:  tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "Epoch:  270   Step:  19   Loss:  tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "Epoch:  280   Step:  19   Loss:  tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "Epoch:  290   Step:  19   Loss:  tensor(0.0195, grad_fn=<MseLossBackward>)\n",
      "Epoch:  300   Step:  19   Loss:  tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "Epoch:  310   Step:  19   Loss:  tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "Epoch:  320   Step:  19   Loss:  tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "Epoch:  330   Step:  19   Loss:  tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "Epoch:  340   Step:  19   Loss:  tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "Epoch:  350   Step:  19   Loss:  tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "Epoch:  360   Step:  19   Loss:  tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "Epoch:  370   Step:  19   Loss:  tensor(0.0132, grad_fn=<MseLossBackward>)\n",
      "Epoch:  380   Step:  19   Loss:  tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "Epoch:  390   Step:  19   Loss:  tensor(0.0206, grad_fn=<MseLossBackward>)\n",
      "Epoch:  400   Step:  19   Loss:  tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "Epoch:  410   Step:  19   Loss:  tensor(0.0063, grad_fn=<MseLossBackward>)\n",
      "Epoch:  420   Step:  19   Loss:  tensor(0.0216, grad_fn=<MseLossBackward>)\n",
      "Epoch:  430   Step:  19   Loss:  tensor(0.0049, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0bb177a833dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Iteration in each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Iterating in each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Removing buffer from previous epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Output training into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get loss for predicted outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs): # Iteration in each epoch\n",
    "    for step, (t_x, t_y) in enumerate(train_loader): # Iterating in each batch\n",
    "        optimizer.zero_grad() # Removing buffer from previous epochs\n",
    "        outputs = net(t_x) # Output training into the model\n",
    "        loss = criterion(outputs, t_y) # Get loss for predicted outputs\n",
    "        if(epoch%10 == 0 and step == train_set/batch_size-1):\n",
    "            print(\"Epoch: \", epoch,\"  Step: \", step,\"  Loss: \", loss)\n",
    "        loss.backward() # Propagate the loss\n",
    "        optimizer.step() # Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = net.eval()\n",
    "test_outputs = net(test_x)\n",
    "mean_test_error = torch.mean(torch.abs(test_y-test_outputs))\n",
    "print(torch.abs(test_y-test_outputs))\n",
    "print(\"Mean Test Error: \", mean_test_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
