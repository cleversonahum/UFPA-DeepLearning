{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.17.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN class for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize, nhiddenLayers, hiddenNeurons):\n",
    "        super(linearRegression, self).__init__()\n",
    "        layers = []\n",
    "\n",
    "        for i in range(0,nhiddenLayers): # adding hidden layers\n",
    "            if(i==0): #Input layer\n",
    "                layers.append(torch.nn.Linear(inputSize, hiddenNeurons))\n",
    "                layers.append(torch.nn.ReLU())\n",
    "            else:\n",
    "                layers.append(torch.nn.BatchNorm1d(hiddenNeurons))\n",
    "                layers.append(torch.nn.Linear(hiddenNeurons, hiddenNeurons))\n",
    "                layers.append(torch.nn.ReLU())\n",
    "\n",
    "        layers.append(torch.nn.Linear(hiddenNeurons, outputSize)) # output layer\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "        print(\"\\nNetwork Architecture: \\n\", self.model,\"\\n\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.001\n",
    "momentum = 0.9 \n",
    "epochs = 200 # Original Value is 2000\n",
    "batch_size = 3000\n",
    "train_set = 60000\n",
    "test_set = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = lambda x: 2 * (x ** 2) - 1\n",
    "func = lambda x: f1(f1(np.cos(x)))\n",
    "x = torch.unsqueeze(torch.FloatTensor(train_set+test_set).uniform_(-2*pi, 2*pi), dim=1)\n",
    "y = func(x)\n",
    "\n",
    "# Train Dataset and making generator for mini-batches\n",
    "train_x = x[0:train_set]\n",
    "train_y = y[0:train_set]\n",
    "train_dataset = Data.TensorDataset(train_x, train_y)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Test Dataset\n",
    "test_x = x[train_set:(train_set+test_set)]\n",
    "test_y = y[train_set:(train_set+test_set)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to obtain training and testing results for a specific network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestNet(arch:tuple) -> torch.tensor:\n",
    "    \n",
    "    # Defining network architecture\n",
    "    print(\"Hidden Layers Quantity: \",arch[0],\",  Units per Layer: \",arch[1],\"\\n\")\n",
    "    net = linearRegression(1, 1, arch[0], arch[1]) # Defining network architecture where arch[1] corresponds to the layer number and arch[2] to the number of units in a hidden layer\n",
    "    criterion = torch.nn.MSELoss() # Define criterion to evaluate the network, in this case MSE\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=learningRate, momentum=momentum) # optimizer used is SGD\n",
    "        \n",
    "    # Training the network\n",
    "    print(\"Training Results: \")\n",
    "    for epoch in range(1,epochs+1): # Iteration in each epoch\n",
    "        for step, (t_x, t_y) in enumerate(train_loader): # Iterating in each mini-batch\n",
    "            optimizer.zero_grad() # Removing buffer from previous epochs\n",
    "            outputs = net(t_x) # Output training into the model\n",
    "            loss = criterion(outputs, t_y) # Get loss for predicted outputs\n",
    "            if((epoch%100==0) and step == train_set/batch_size-1): #Print Train results for each 100 epochs\n",
    "                print(\"Epoch: \", epoch,\"  Step: \", step,\"  Loss: \", loss)\n",
    "            loss.backward() # Propagate the loss\n",
    "            optimizer.step() # Update parameters\n",
    "            \n",
    "    # Evaluating the network using the test dataset\n",
    "    print(\"\\n\\nEvaluation over test dataset:\")\n",
    "    test_outputs = net(test_x) # Generating network outputs for test data\n",
    "    mean_test_error = torch.sqrt(criterion(test_outputs, test_y)) # Calculating RMSE over the predicted data\n",
    "    print(\"Test Error (RMSE): \", mean_test_error, \"\\n\\n\\n\\n\")\n",
    "    \n",
    "    return mean_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = trainTestNet((1,256))\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining network architectures deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "net_archs = [(1,24), (2, 36), (3,24)] # The first value of a tuple corresponds to the number of hidden layers and the other value to number of units/layer\n",
    "times = 5 # Repeating each architecture result N times\n",
    "results = torch.zeros([len(net_archs), times])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layers Quantity:  1 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4901, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.4819, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.6900, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  2 ,  Units per Layer:  36 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=36, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=36, out_features=36, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=36, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.3968, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.3268, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.5696, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  3 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.0133, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.0984, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  1 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4905, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.4699, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.6934, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  2 ,  Units per Layer:  36 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=36, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=36, out_features=36, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=36, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4134, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.3244, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.5730, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  3 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.1462, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.0608, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.2136, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  1 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4859, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.4684, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.6892, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  2 ,  Units per Layer:  36 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=36, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=36, out_features=36, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=36, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4607, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.3364, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.5835, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  3 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.1516, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.0355, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.1363, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  1 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4837, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.4765, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.6880, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  2 ,  Units per Layer:  36 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=36, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=36, out_features=36, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=36, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4568, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.3860, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.6234, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  3 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.1333, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.0326, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.1965, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  1 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4889, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.4851, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.6914, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  2 ,  Units per Layer:  36 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=36, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=36, out_features=36, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=36, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.4757, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.4280, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.6461, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hidden Layers Quantity:  3 ,  Units per Layer:  24 \n",
      "\n",
      "\n",
      "Network Architecture: \n",
      " Sequential(\n",
      "  (0): Linear(in_features=1, out_features=24, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): BatchNorm1d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): Linear(in_features=24, out_features=24, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=24, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Training Results: \n",
      "Epoch:  100   Step:  19   Loss:  tensor(0.1182, grad_fn=<MseLossBackward>)\n",
      "Epoch:  200   Step:  19   Loss:  tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "\n",
      "\n",
      "Evaluation over test dataset:\n",
      "Test Error (RMSE):  tensor(0.1489, grad_fn=<SqrtBackward>) \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(times): # Repeating the process N times\n",
    "    for index, arch in enumerate(net_archs): # Iterating over all architectures\n",
    "        results[index, t] = trainTestNet(arch)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6900, 0.6934, 0.6892, 0.6880, 0.6914],\n",
      "        [0.5696, 0.5730, 0.5835, 0.6234, 0.6461],\n",
      "        [0.0984, 0.2136, 0.1363, 0.1965, 0.1489]], grad_fn=<CopySlices>)\n",
      "torch.return_types.min(\n",
      "values=tensor([0.6880, 0.5696, 0.0984], grad_fn=<MinBackward0>),\n",
      "indices=tensor([3, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(torch.min(results, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "fig1.set_title(\"Text Erro x Number of Units\")\n",
    "fig1.set_xlabel(\"Number of Units\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
